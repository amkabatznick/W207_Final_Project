{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Project w207\n",
    "\n",
    "Matthew Holmes\n",
    "Andrew Kabatznick\n",
    "Grant Whittington\n",
    "\n",
    "Data from: https://www.kaggle.com/c/bike-sharing-demand/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "##basically just importanting most of the libraries from previous assignments\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_raw_data(raw_input):\n",
    "    #Putting Data into Pandas dataframe\n",
    "        Column_Headers = csvfile.readline().split(',')\n",
    "        Column_Headers[-1] = \"totalcount\"\n",
    "        csvfile.next()\n",
    "        train_data = []\n",
    "        for row in csvfile:\n",
    "            new_row = row.replace('\\n', '')\n",
    "            train_data.append(new_row.split(','))\n",
    "        \n",
    "        df = pd.DataFrame(data = train_data[1:-1], columns = Column_Headers)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train.csv') as csvfile:\n",
    "    train_data = transform_raw_data(csvfile)\n",
    "with open('test.csv') as csvfile:\n",
    "    test_data = transform_raw_data(csvfile)\n",
    "a = train_data['datetime']\n",
    "[a,b] = a.iloc[1].split()\n",
    "[c,d,e]=a.split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Need to do train/dev split, also modify/add any features we desire\n",
    "def modify_factors(input_data):\n",
    "    output_data=pd.DataFrame(input_data)\n",
    "    ##Split datetime into year, month, day, and hour columns\n",
    "    rawdate = input_data['datetime']\n",
    "    for i in range(len(rawdate)):\n",
    "        [date,hour] = rawdate.iloc[i].split()\n",
    "        [year,month,day] = date.split(\"-\")\n",
    "        output_data['year'] = int(year)\n",
    "        output_data['month']=int(month)\n",
    "        output_data['day']=int(day)\n",
    "        output_data['hour']=int(hour[:2])\n",
    "    ##One hot encoding for season, weather\n",
    "    season_onehot = pd.get_dummies(input_data['season'])\n",
    "    season_onehot.columns = [\"winter\", \"spring\", \"summer\", \"autumn\"]\n",
    "    output_data = pd.concat([output_data,season_onehot],axis=1)\n",
    "    \n",
    "    weather_onehot = pd.get_dummies(input_data['weather'])\n",
    "    weather_onehot.columns = [\"clear\", \"misty\", \"light_precip\", \"heavy_precip\"]\n",
    "    output_data = pd.concat([output_data,weather_onehot],axis=1)\n",
    "    \n",
    "    del output_data['datetime']\n",
    "    del output_data['season']\n",
    "    del output_data['weather']\n",
    "    \n",
    "    return output_data\n",
    "\n",
    "train_data = modify_factors(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting Columns into Prediction and Numerical vs Categorical\n",
    "All_Columns = list(train_data.columns.values)\n",
    "Prediction_Columns = All_Columns[:]\n",
    "Prediction_Columns.remove('registered')\n",
    "Prediction_Columns.remove('casual')\n",
    "Prediction_Columns.remove('totalcount')\n",
    "Numerical_Predictors = Prediction_Columns[0:9 or None]\n",
    "Categorical_Predictors = Prediction_Columns[10:-0 or None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, dev, test split of base data\n",
    "def data_split(data,train_percent,dev_percent,is_random):\n",
    "    size_of_data=len(data)\n",
    "    train_amount=int(round(train_percent*size_of_data))\n",
    "    dev_amount=int(round(dev_percent*size_of_data))\n",
    "    train, dev, test = np.split(data.sample(frac=1), [train_amount, train_amount+dev_amount])\n",
    "    return train,dev,test\n",
    "[train_set,dev_set,test_set]=data_split(train_data,.45,.3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.274572154876\n"
     ]
    }
   ],
   "source": [
    "#Basic Linear Regression using numerical predictors\n",
    "LinearReg = LinearRegression()\n",
    "LinearReg.fit(train_set[Prediction_Columns], train_set['totalcount'])\n",
    "print LinearReg.score(dev_set[Prediction_Columns], dev_set['totalcount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Investigate using bins for totalcount to improve below methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00857580398162\n"
     ]
    }
   ],
   "source": [
    "#Multinomial Naive Bayes\n",
    "MultiBayes = MultinomialNB()\n",
    "MultiBayes.fit(train_set[Prediction_Columns], train_set['totalcount'])\n",
    "print MultiBayes.score(dev_set[Prediction_Columns], dev_set['totalcount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00704441041348\n"
     ]
    }
   ],
   "source": [
    "#Decision Trees\n",
    "DTree = tree.DecisionTreeClassifier()\n",
    "DTree.fit(train_set[Prediction_Columns], train_set['totalcount'])\n",
    "print DTree.score(dev_set[Prediction_Columns], dev_set['totalcount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Need to investigate a better way to score these things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pruning?  Worth doing at all?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
